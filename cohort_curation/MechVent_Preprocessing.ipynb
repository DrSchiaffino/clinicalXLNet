{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/ipython-6.2.1-py3.6.egg/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_notes = pd.read_csv('/scratch/kh2383/MIMIC/1.4/NOTEEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm = pd.read_csv('/scratch/kh2383/MIMIC/1.4/ADMISSIONS.csv')\n",
    "df_icu = pd.read_csv('/scratch/kh2383/MIMIC/1.4/ICUSTAYS.csv')\n",
    "df_mechvent_d2 = pd.read_csv('../Data/d2_mechvent_cohort08Oct19.csv')\n",
    "df_mechvent_d7 = pd.read_csv('../Data/d7_mechvent_cohort27Sep19.csv')\n",
    "df_mechvent_d14 = pd.read_csv('../Data/d14_mechvent_cohort27Sep19.csv')\n",
    "df_mechvent_entire = pd.read_csv('../Data/entire_mechvent_cohort_starttimes15Oct19.csv')\n",
    "\n",
    "# add the INTIME Column from ICUSTAYS table\n",
    "df_mechvent_d2 = pd.merge(df_mechvent_d2, df_icu[['ICUSTAY_ID', 'INTIME']], on = ['ICUSTAY_ID'], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    '''\n",
    "    Input\n",
    "        df: the initial cohort\n",
    "        \n",
    "    output\n",
    "        df_less_n_r: first icu stay cohort with notes within 48 hour starting from first vent time, with the following labels:\n",
    "            COHORT:\n",
    "                0 for not prolonged, 1 for more than 7 days, 2 for more than 14 days. So for prolonged for more than 7 days, should sum 1 & 2 cohort\n",
    "            LABEL:\n",
    "                0 for not prolonged, 1 for more than 7 days (including more than 14 days)\n",
    "            DEATH_90:\n",
    "                0 for not dead, 1 for dead within 90 days of 48 hours (action time) after first vent time \n",
    "            DAYS_UNTIL_DEATH: \n",
    "                continuous variable of days of death from the \n",
    "            \n",
    "            this table only returns notes that are in the respiratory, nurses, nurse/others category\n",
    "       \n",
    "       df_less_n: the above table with all the categories\n",
    "       \n",
    "       df_physician: the tables for admissions that have physician's only. It includes four categories: physician, respiratory, nurses, nurse/others category\n",
    "       \n",
    "       df_notes_cohort: the above table with no 48h restriction\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    # only use the first ICU stays STARTING from the FIRST VENT TIME\n",
    "    df = df.sort_values(['HADM_ID','INTIME_x']).groupby('HADM_ID', as_index=False).first()\n",
    "    \n",
    "    # drop all the repetitive ventilation events, so now this df has each row correspond to unique one admission's first icu stay\n",
    "    df = df[['ICUSTAY_ID', 'HADM_ID', 'ADMITTIME','DISCHTIME','FIRST_VENT_STARTTIME', 'DOD']].drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    df.DOD = pd.to_datetime(df.DOD, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    df.ADMITTIME = pd.to_datetime(df.ADMITTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    df.DISCHTIME = pd.to_datetime(df.DISCHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    df.FIRST_VENT_STARTTIME = pd.to_datetime(df.FIRST_VENT_STARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "    # cohort 0: not prolonged, 1: more than 7 days, 2: more than 14 days\n",
    "    df = df.assign(COHORT = [1 if i else 0 for i in (df.HADM_ID.isin(df_mechvent_d7.HADM_ID.unique()))])\n",
    "    df.loc[df.HADM_ID.isin(df_mechvent_d14.HADM_ID.unique()), 'COHORT'] = 2\n",
    "    count = df.groupby('COHORT').HADM_ID.nunique().values.tolist()\n",
    "    print('Not prolonged: {}, more than 7 days: {}, more than 14 days: {}'.format(count[0], count[1], count[2]))\n",
    "    \n",
    "    #to calculate the death label\n",
    "    df = df.assign(DAYS_UNTIL_DEATH = ((df['DOD']-df['FIRST_VENT_STARTTIME']).dt.total_seconds()/(60*60*24)))\n",
    "    df = df.assign(DEATH = [1 if i else 0 for i in (df.DAYS_UNTIL_DEATH < 7)])\n",
    "    df = df.assign(DEATH_90 = [1 if i else 0 for i in (df.DAYS_UNTIL_DEATH < 92)])\n",
    "    \n",
    "    df_curated = df.drop_duplicates().reset_index(drop = True)\n",
    "    print('n (number of admissions) is : {}'.format(len(df_curated.HADM_ID.unique())))\n",
    "    \n",
    "    # filter so that notes only in the admission cohort, in the CATEGORY we want\n",
    "    df_notes_cohort = df_notes[df_notes.HADM_ID.isin(df_curated.HADM_ID)]\n",
    "    print('removed subjects with no notes associated, n: {}'.format(len(df_notes_cohort.HADM_ID.unique())))\n",
    "\n",
    "    df_notes_cohort = pd.merge(df_notes_cohort[['HADM_ID', 'CHARTTIME', 'TEXT', 'CATEGORY', 'DESCRIPTION']], df_curated, on = ['HADM_ID'], how = 'inner')\n",
    "    print('after merge, n: {}'.format(len(df_notes_cohort.HADM_ID.unique())))\n",
    "    \n",
    "    df_notes_cohort.CHARTTIME = pd.to_datetime(df_notes_cohort.CHARTTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "    df_notes_cohort.ADMITTIME = pd.to_datetime(df_notes_cohort.ADMITTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "        \n",
    "    TIME_to_VENT = ((df_notes_cohort['CHARTTIME']-df_notes_cohort['FIRST_VENT_STARTTIME']).dt.total_seconds()/(60*60))\n",
    "    df_less_n = df_notes_cohort[ (TIME_to_VENT<=48) & (TIME_to_VENT >=0)]\n",
    "    print('restricted to the first 48h, n: {}'.format(len(df_less_n.HADM_ID.unique())))\n",
    "    df_less_n = df_less_n.assign(Label = [1 if i else 0 for i in (df_less_n.COHORT != 0)])\n",
    "\n",
    "    # create cohort that have physician notes aka 2008-2013 MIMIC\n",
    "    df_physician = df_less_n[df_less_n.CATEGORY.isin(['Physician ','Nursing', 'Nursing/other', 'Respiratory '])]\n",
    "    df_physician = df_physician[df_physician.HADM_ID.isin(df_physician[df_physician.CATEGORY == 'Physician '].HADM_ID.unique())]\n",
    "    \n",
    "    print('There are {} # of admissions that have physician notes'.format(len(df_physician.HADM_ID.unique())))\n",
    "    \n",
    "    df_less_n_r = df_less_n[df_less_n.CATEGORY.isin(['Nursing', 'Nursing/other', 'Respiratory '])]\n",
    "\n",
    "    print('remove subjects with no notes in the selected categories the entire admission, n: {}'.format(len(df_notes_cohort[df_notes_cohort.CATEGORY.isin(['Physician ', 'Nursing', 'Nursing/other'])].HADM_ID.unique())))\n",
    "    print('remove subjects with no notes in the selected categories under 48h, n: {}'.format(len(df_less_n_r.HADM_ID.unique())))\n",
    "    \n",
    "    print('# of notes restricted to the three categories: {}'.format(len(df_less_n_r)))\n",
    "    print('# of notes using all categories: {}'.format(len(df_less_n)))\n",
    "    \n",
    "    print('--- for prediction: below are label stats using three categories only ---')\n",
    "    \n",
    "    count = df_less_n_r.groupby('COHORT').HADM_ID.nunique().values.tolist()\n",
    "    print('Not prolonged: {}, prolonged: {}, out of which, more than 7 days: {}, more than 14 days: {}'.format(count[0], count[1]+count[2], count[1], count[2]))\n",
    "    count = df_less_n_r.groupby('DEATH').HADM_ID.nunique().values.tolist()\n",
    "    print('Not Death within 7 days: {}, Death within 7 days: {}'.format(count[0], count[1]))\n",
    "    \n",
    "    df_death = df_less_n_r[df_less_n_r.HADM_ID.isin(df_less_n_r.groupby('DEATH').HADM_ID.unique()[1])]\n",
    "    c = df_death[['HADM_ID','COHORT']].drop_duplicates().COHORT.value_counts().values\n",
    "    try:\n",
    "        print('Out of the {} death within 7 days, {} is from cohort not prolonged, {} is from cohort prolonged more than 7 days, and {} for 14 days'.format(count[1], c[0], c[1], c[2]))\n",
    "    except:\n",
    "        print('Out of the {} death within 7 days, {} is from cohort not prolonged, {} is from cohort prolonged more than 7 days, and 0 for 14 days'.format(count[1], c[0], c[1]))\n",
    "        \n",
    "    count = df_less_n_r.groupby('DEATH_90').HADM_ID.nunique().values.tolist()\n",
    "    print('Not Death within 90 days: {}, Death within 90 days: {}'.format(count[0], count[1]))\n",
    "        \n",
    "    return df_less_n_r.reset_index(drop = True), df_less_n.reset_index(drop = True), df_physician.reset_index(drop = True), df_notes_cohort.reset_index(drop = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not prolonged: 5185, more than 7 days: 2424, more than 14 days: 1874\n",
      "n (number of admissions) is : 9483\n",
      "removed subjects with no notes associated, n: 9372\n",
      "after merge, n: 9372\n",
      "restricted to the first 48h, n: 9095\n",
      "There are 1538 # of admissions that have physician notes\n",
      "remove subjects with no notes in the selected categories the entire admission, n: 7290\n",
      "remove subjects with no notes in the selected categories under 48h, n: 7287\n",
      "# of notes restricted to the three categories: 71439\n",
      "# of notes using all categories: 120108\n",
      "--- for prediction: below are label stats using three categories only ---\n",
      "Not prolonged: 3875, prolonged: 3412, out of which, more than 7 days: 1858, more than 14 days: 1554\n",
      "Not Death within 7 days: 6360, Death within 7 days: 927\n",
      "Out of the 927 death within 7 days, 885 is from cohort not prolonged, 42 is from cohort prolonged more than 7 days, and 0 for 14 days\n",
      "Not Death within 90 days: 4607, Death within 90 days: 2680\n"
     ]
    }
   ],
   "source": [
    "df_less_n_d2, df_less_48, df_physician, df_notes_d2 = process(df_mechvent_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess1(x):\n",
    "    y=re.sub('\\\\[(.*?)\\\\]','',x) #remove de-identified brackets\n",
    "    y=re.sub('[0-9]+\\.','',y) #remove 1.2. since the segmenter segments based on this\n",
    "    y=re.sub('dr\\.','doctor',y)\n",
    "    y=re.sub('m\\.d\\.','md',y)\n",
    "    y=re.sub('--|__|==','',y)\n",
    "    \n",
    "    # remove punctuation, digits, spaces\n",
    "    #y = y.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    y = y.translate(str.maketrans(\"\", \"\", string.digits))\n",
    "    y = \" \".join(y.split())\n",
    "    return y\n",
    "\n",
    "def preprocessing(df_less_n, max_seq_len): \n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].fillna(' ')\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].str.replace('\\n',' ')\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].str.replace('\\r',' ')\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].apply(str.strip)\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].str.lower()\n",
    "\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].apply(lambda x: preprocess1(x))\n",
    "    df_less_n['# of tokens'] = df_less_n['TEXT'].str.split().str.len()\n",
    "    df_less_n = df_less_n[df_less_n['# of tokens'] > 5]\n",
    "    \n",
    "    df_concat = pd.DataFrame(df_less_n.groupby('HADM_ID')['TEXT'].apply(lambda x: \"%s\" % ' '.join(x))).reset_index()\n",
    "    df_concat = df_concat.assign(Label = df_concat['HADM_ID'].apply(lambda x: df_less_n[df_less_n['HADM_ID']==x].Label.values[0]))\n",
    "    df_concat = df_concat.assign(DEATH_90 = df_concat['HADM_ID'].apply(lambda x: df_less_n[df_less_n['HADM_ID']==x].DEATH_90.values[0]))\n",
    "\n",
    "    df_less_n = df_concat\n",
    "    df_len = len(df_less_n)\n",
    "\n",
    "    want=pd.DataFrame({'HADM_ID':[],'TEXT':[],'Label':[],'DEATH_90':[]})\n",
    "    for i in tqdm(range(df_len)):\n",
    "        x=df_less_n.TEXT.iloc[i].split()\n",
    "        n=int(len(x)/max_seq_len)\n",
    "        for j in range(n):\n",
    "            want=want.append({'TEXT':' '.join(x[j*max_seq_len:(j+1)*max_seq_len]),'Label':df_less_n.Label.iloc[i],'DEATH_90': df_less_n.DEATH_90.iloc[i], 'HADM_ID':df_less_n.HADM_ID.iloc[i]},ignore_index=True)\n",
    "        if len(x)%max_seq_len>100:\n",
    "            want=want.append({'TEXT':' '.join(x[-(len(x)%max_seq_len):]),'Label':df_less_n.Label.iloc[i],'DEATH_90': df_less_n.DEATH_90.iloc[i],'HADM_ID':df_less_n.HADM_ID.iloc[i]},ignore_index=True)\n",
    "        \n",
    "    return want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7287/7287 [02:37<00:00, 46.26it/s]\n"
     ]
    }
   ],
   "source": [
    "df_use_max_len = preprocessing(df_less_n_d2, 318)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:23<00:00,  4.63s/it]\n"
     ]
    }
   ],
   "source": [
    "path = DATAFOLDER_PATH\n",
    "# create one untouchable test set first (instead of create five) just to enlarge the pretraining notes size\n",
    "df_HADM_ID = df_use_max_len.HADM_ID.drop_duplicates().reset_index(drop = True)\n",
    "test_id = df_HADM_ID.sample(frac = 0.1, replace = False, random_state = 1)\n",
    "test = df_use_max_len[df_use_max_len.HADM_ID.isin(test_id.values)]\n",
    "test.reset_index(drop = True).to_csv(path + '/test.csv')\n",
    "\n",
    "train_val_id = df_HADM_ID[~df_HADM_ID.index.isin(test_id.index)]\n",
    "\n",
    "for i in tqdm(range(1,6)):\n",
    "    val_id = train_val_id.sample(frac = 1/9, replace = False, random_state = i)\n",
    "    train_id = train_val_id[~train_val_id.index.isin(val_id.index)]\n",
    "        \n",
    "    train = df_use_max_len[df_use_max_len.HADM_ID.isin(train_id.values)]\n",
    "    val = df_use_max_len[df_use_max_len.HADM_ID.isin(val_id.values)]\n",
    "    train_val = df_use_max_len[df_use_max_len.HADM_ID.isin(train_val_id)]\n",
    "    \n",
    "    train.reset_index(drop = True).to_csv(path + '/train.csv')\n",
    "    val.reset_index(drop = True).to_csv(path + '/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
